{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScrapping ATC/DDD data from WHO Collaborating Centre for Drug Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ATC root: A\n",
      "Building who_atc_A.\n",
      "Scraping https://www.whocc.no/atc_ddd_index/?code=A&showdescription=no.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "scrape_atc_data() only accepts single objects, not vectors. Please provide a single valid ATC code as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atc_root \u001b[38;5;129;01min\u001b[39;00m atc_root_codes:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing ATC root: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00matc_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m     \u001b[43mcache_or_generate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwho_atc_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43matc_root\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscrape_atc_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matc_root\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Read the files produced by scrape_atc_data()\u001b[39;00m\n\u001b[0;32m    110\u001b[0m combined_atc_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([load_cached_data(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho_atc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00matc_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m atc_root \u001b[38;5;129;01min\u001b[39;00m atc_root_codes \u001b[38;5;28;01mif\u001b[39;00m load_cached_data(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho_atc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00matc_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m, in \u001b[0;36mcache_or_generate_data\u001b[1;34m(var_name, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuilding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m     var_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed. Saving to file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m     var_val\u001b[38;5;241m.\u001b[39mto_pickle(cache_file)\n",
      "Cell \u001b[1;32mIn[3], line 78\u001b[0m, in \u001b[0;36mscrape_atc_data\u001b[1;34m(root_atc_code)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scraped_strings:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m tval \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124matc_code\u001b[39m\u001b[38;5;124m'\u001b[39m: [s\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m]], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124matc_name\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(s\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m1\u001b[39m:])]}), \u001b[43mscrape_atc_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scraped_strings], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m atc_code_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     81\u001b[0m     root_atc_code_name \u001b[38;5;241m=\u001b[39m html_data\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#content a\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mget_text()\n",
      "Cell \u001b[1;32mIn[3], line 63\u001b[0m, in \u001b[0;36mscrape_atc_data\u001b[1;34m(root_atc_code)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_atc_data\u001b[39m(root_atc_code):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root_atc_code, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(root_atc_code) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscrape_atc_data() only accepts single objects, not vectors. Please provide a single valid ATC code as input.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m     web_address \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.whocc.no/atc_ddd_index/?code=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_atc_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&showdescription=no\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweb_address\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: scrape_atc_data() only accepts single objects, not vectors. Please provide a single valid ATC code as input."
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Function to check if a library is installed, and install it if not\n",
    "def install_and_import(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    finally:\n",
    "        globals()[package] = importlib.import_module(package)\n",
    "\n",
    "# List of required packages\n",
    "required_packages = ['os', 'requests', 'pandas', 'bs4', 'datetime', 'lxml']\n",
    "\n",
    "# Install and import required packages\n",
    "for package in required_packages:\n",
    "    install_and_import(package)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure directory exists\n",
    "def create_directory_if_not_exists(*paths):\n",
    "    dir_path = os.path.join(*paths)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n",
    "\n",
    "# Create output directories if they do not exist\n",
    "output_directory = create_directory_if_not_exists('output')\n",
    "cache_directory = create_directory_if_not_exists(output_directory, 'cache')\n",
    "\n",
    "# Function to wrap RDS equivalent in Python\n",
    "def cache_or_generate_data(var_name, func, *args, **kwargs):\n",
    "    cache_file = os.path.join(cache_directory, f'{var_name}.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Reading '{var_name}' from file '{cache_file}'... \")\n",
    "        var_val = pd.read_pickle(cache_file)\n",
    "    else:\n",
    "        print(f'Building {var_name}.')\n",
    "        var_val = func(*args, **kwargs)\n",
    "        print(f\"{var_name} completed. Saving to file '{cache_file}'... \")\n",
    "        var_val.to_pickle(cache_file)\n",
    "    return var_val\n",
    "\n",
    "# Function to get RDS equivalent in Python\n",
    "def load_cached_data(var_name):\n",
    "    cache_file = os.path.join(cache_directory, f'{var_name}.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Reading '{var_name}' from file '{cache_file}'... \")\n",
    "        return pd.read_pickle(cache_file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Unable to find file {cache_file}.')\n",
    "\n",
    "# Scrape data from WHO ATC website\n",
    "def scrape_atc_data(root_atc_code):\n",
    "    if not isinstance(root_atc_code, str) or len(root_atc_code) != 1:\n",
    "        raise ValueError('scrape_atc_data() only accepts single objects, not vectors. Please provide a single valid ATC code as input.')\n",
    "    \n",
    "    web_address = f'https://www.whocc.no/atc_ddd_index/?code={root_atc_code}&showdescription=no'\n",
    "    print(f'Scraping {web_address}.')\n",
    "    atc_code_length = len(root_atc_code)\n",
    "    response = requests.get(web_address)\n",
    "    html_data = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    if atc_code_length < 5:\n",
    "        scraped_strings = html_data.select_one(\"#content > p:nth-of-type(2n)\").get_text().split('\\n')\n",
    "        scraped_strings = list(filter(len, scraped_strings))\n",
    "        \n",
    "        if not scraped_strings:\n",
    "            return None\n",
    "        \n",
    "        tval = pd.concat([pd.concat([pd.DataFrame({'atc_code': [s.split()[0]], 'atc_name': [' '.join(s.split()[1:])]}), scrape_atc_data(s.split()[0])], ignore_index=True) for s in scraped_strings], ignore_index=True)\n",
    "        \n",
    "        if atc_code_length == 1:\n",
    "            root_atc_code_name = html_data.select(\"#content a\")[2].get_text()\n",
    "            return pd.concat([pd.DataFrame({'atc_code': [root_atc_code], 'atc_name': [root_atc_code_name]}), tval], ignore_index=True)\n",
    "        else:\n",
    "            return tval\n",
    "    else:\n",
    "        table = html_data.select_one(\"ul > table\")\n",
    "        if table is None:\n",
    "            return None\n",
    "        \n",
    "        df = pd.read_html(str(table), header=0)[0]\n",
    "        df.columns = ['atc_code', 'atc_name', 'ddd', 'uom', 'adm_r', 'note']\n",
    "        df = df.applymap(lambda x: None if x == '' else x)\n",
    "        \n",
    "        for i in range(1, len(df)):\n",
    "            if pd.isna(df.atc_code[i]):\n",
    "                df.atc_code[i] = df.atc_code[i-1]\n",
    "                df.atc_name[i] = df.atc_name[i-1]\n",
    "        \n",
    "        return df\n",
    "\n",
    "# List of root ATC codes to scrape\n",
    "atc_root_codes = ['A', 'B', 'C', 'D', 'G', 'H', 'J', 'L', 'M', 'N', 'P', 'R', 'S', 'V']\n",
    "\n",
    "# Process each ATC root code individually\n",
    "for atc_root in atc_root_codes:\n",
    "    print(f'Processing ATC root: {atc_root}')\n",
    "    cache_or_generate_data(f'who_atc_{atc_root}', scrape_atc_data, atc_root)\n",
    "\n",
    "# Read the files produced by scrape_atc_data()\n",
    "combined_atc_data = pd.concat([load_cached_data(f'who_atc_{atc_root}') for atc_root in atc_root_codes if load_cached_data(f'who_atc_{atc_root}') is not None], ignore_index=True)\n",
    "\n",
    "# Write them to a CSV file. Generate file name from current date in year-month-day format.\n",
    "output_file_name = os.path.join(output_directory, f'WHO ATC-DDD {datetime.now().strftime(\"%Y-%m-%d\")}.csv')\n",
    "print(f'Writing results to {output_file_name}.')\n",
    "if os.path.exists(output_file_name):\n",
    "    print('Warning: file already exists. Will be overwritten.')\n",
    "combined_atc_data.to_csv(output_file_name, index=False)\n",
    "\n",
    "# Finish execution\n",
    "print('Script execution completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure directory exists\n",
    "def create_directory_if_not_exists(*paths):\n",
    "    dir_path = os.path.join(*paths)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n",
    "\n",
    "# Create output directories if they do not exist\n",
    "output_directory = create_directory_if_not_exists('output')\n",
    "cache_directory = create_directory_if_not_exists(output_directory, 'cache')\n",
    "\n",
    "# Function to wrap RDS equivalent in Python\n",
    "def cache_or_generate_data(var_name, func, *args, **kwargs):\n",
    "    cache_file = os.path.join(cache_directory, f'{var_name}.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Reading '{var_name}' from file '{cache_file}'... \")\n",
    "        var_val = pd.read_pickle(cache_file)\n",
    "    else:\n",
    "        print(f'Building {var_name}.')\n",
    "        var_val = func(*args, **kwargs)\n",
    "        print(f\"{var_name} completed. Saving to file '{cache_file}'... \")\n",
    "        var_val.to_pickle(cache_file)\n",
    "    return var_val\n",
    "\n",
    "# Function to get RDS equivalent in Python\n",
    "def load_cached_data(var_name):\n",
    "    cache_file = os.path.join(cache_directory, f'{var_name}.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Reading '{var_name}' from file '{cache_file}'... \")\n",
    "        return pd.read_pickle(cache_file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Unable to find file {cache_file}.')\n",
    "\n",
    "# Scrape data from WHO ATC website\n",
    "def scrape_atc_data(root_atc_code):\n",
    "    if not isinstance(root_atc_code, str) or len(root_atc_code) != 1:\n",
    "        raise ValueError('scrape_atc_data() only accepts single objects, not vectors. Please provide a single valid ATC code as input.')\n",
    "    \n",
    "    web_address = f'https://www.whocc.no/atc_ddd_index/?code={root_atc_code}&showdescription=no'\n",
    "    print(f'Scraping {web_address}.')\n",
    "    atc_code_length = len(root_atc_code)\n",
    "    response = requests.get(web_address)\n",
    "    html_data = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    if atc_code_length < 5:\n",
    "        scraped_strings = html_data.select_one(\"#content > p:nth-of-type(2n)\").get_text().split('\\n')\n",
    "        scraped_strings = list(filter(len, scraped_strings))\n",
    "        \n",
    "        if not scraped_strings:\n",
    "            return None\n",
    "        \n",
    "        tval = pd.concat([pd.concat([pd.DataFrame({'atc_code': [s.split()[0]], 'atc_name': [' '.join(s.split()[1:])]}), scrape_atc_data(s.split()[0])], ignore_index=True) for s in scraped_strings], ignore_index=True)\n",
    "        \n",
    "        if atc_code_length == 1:\n",
    "            root_atc_code_name = html_data.select(\"#content a\")[2].get_text()\n",
    "            return pd.concat([pd.DataFrame({'atc_code': [root_atc_code], 'atc_name': [root_atc_code_name]}), tval], ignore_index=True)\n",
    "        else:\n",
    "            return tval\n",
    "    else:\n",
    "        table = html_data.select_one(\"ul > table\")\n",
    "        if table is None:\n",
    "            return None\n",
    "        \n",
    "        df = pd.read_html(str(table), header=0)[0]\n",
    "        df.columns = ['atc_code', 'atc_name', 'ddd', 'uom', 'adm_r', 'note']\n",
    "        df = df.applymap(lambda x: None if x == '' else x)\n",
    "        \n",
    "        for i in range(1, len(df)):\n",
    "            if pd.isna(df.atc_code[i]):\n",
    "                df.atc_code[i] = df.atc_code[i-1]\n",
    "                df.atc_name[i] = df.atc_name[i-1]\n",
    "        \n",
    "        return df\n",
    "\n",
    "# List of root ATC codes to scrape\n",
    "atc_root_codes = ['A', 'B', 'C', 'D', 'G', 'H', 'J', 'L', 'M', 'N', 'P', 'R', 'S', 'V']\n",
    "\n",
    "# Process each ATC root code individually\n",
    "for atc_root in atc_root_codes:\n",
    "    print(f'Processing ATC root: {atc_root}')\n",
    "    cache_or_generate_data(f'who_atc_{atc_root}', scrape_atc_data, str(atc_root))\n",
    "\n",
    "# Read the files produced by scrape_atc_data()\n",
    "combined_atc_data = pd.concat([load_cached_data(f'who_atc_{atc_root}') for atc_root in atc_root_codes if load_cached_data(f'who_atc_{atc_root}') is not None], ignore_index=True)\n",
    "\n",
    "# Write them to a CSV file. Generate file name from current date in year-month-day format.\n",
    "output_file_name = os.path.join(output_directory, f'WHO ATC-DDD {datetime.now().strftime(\"%Y-%m-%d\")}.csv')\n",
    "print(f'Writing results to {output_file_name}.')\n",
    "if os.path.exists(output_file_name):\n",
    "    print('Warning: file already exists. Will be overwritten.')\n",
    "combined_atc_data.to_csv(output_file_name, index=False)\n",
    "\n",
    "# Finish execution\n",
    "print('Script execution completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure directory exists\n",
    "def create_directory_if_not_exists(*paths):\n",
    "    dir_path = os.path.join(*paths)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n",
    "\n",
    "# Create output directories if they do not exist\n",
    "output_directory = create_directory_if_not_exists('output')\n",
    "cache_directory = create_directory_if_not_exists(output_directory, 'cache')\n",
    "\n",
    "# Function to wrap RDS equivalent in Python\n",
    "def cache_or_generate_data(var_name, func, *args, **kwargs):\n",
    "    cache_file = os.path.join(cache_directory, f'{var_name}.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Reading '{var_name}' from file '{cache_file}'... \")\n",
    "        var_val = pd.read_pickle(cache_file)\n",
    "    else:\n",
    "        print(f'Building {var_name}.')\n",
    "        var_val = func(*args, **kwargs)\n",
    "        print(f\"{var_name} completed. Saving to file '{cache_file}'... \")\n",
    "        var_val.to_pickle(cache_file)\n",
    "    return var_val\n",
    "\n",
    "# Function to get RDS equivalent in Python\n",
    "def load_cached_data(var_name):\n",
    "    cache_file = os.path.join(cache_directory, f'{var_name}.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Reading '{var_name}' from file '{cache_file}'... \")\n",
    "        return pd.read_pickle(cache_file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Unable to find file {cache_file}.')\n",
    "\n",
    "# Scrape data from WHO ATC website\n",
    "def scrape_atc_data(root_atc_code):\n",
    "    if not isinstance(root_atc_code, str) or len(root_atc_code) != 1:\n",
    "        raise ValueError('scrape_atc_data() only accepts single objects, not vectors. Please provide a single valid ATC code as input.')\n",
    "    \n",
    "    web_address = f'https://www.whocc.no/atc_ddd_index/?code={root_atc_code}&showdescription=no'\n",
    "    print(f'Scraping {web_address}.')\n",
    "    atc_code_length = len(root_atc_code)\n",
    "    response = requests.get(web_address)\n",
    "    html_data = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    if atc_code_length < 5:\n",
    "        scraped_strings = html_data.select_one(\"#content > p:nth-of-type(2n)\").get_text().split('\\n')\n",
    "        scraped_strings = list(filter(len, scraped_strings))\n",
    "        \n",
    "        if not scraped_strings:\n",
    "            return None\n",
    "        \n",
    "        tval = pd.concat([pd.concat([pd.DataFrame({'atc_code': [s.split()[0]], 'atc_name': [' '.join(s.split()[1:])]}), scrape_atc_data(s.split()[0])], ignore_index=True) for s in scraped_strings], ignore_index=True)\n",
    "        \n",
    "        if atc_code_length == 1:\n",
    "            root_atc_code_name = html_data.select(\"#content a\")[2].get_text()\n",
    "            return pd.concat([pd.DataFrame({'atc_code': [root_atc_code], 'atc_name': [root_atc_code_name]}), tval], ignore_index=True)\n",
    "        else:\n",
    "            return tval\n",
    "    else:\n",
    "        table = html_data.select_one(\"ul > table\")\n",
    "        if table is None:\n",
    "            return None\n",
    "        \n",
    "        df = pd.read_html(str(table), header=0)[0]\n",
    "        df.columns = ['atc_code', 'atc_name', 'ddd', 'uom', 'adm_r', 'note']\n",
    "        df = df.applymap(lambda x: None if x == '' else x)\n",
    "        \n",
    "        for i in range(1, len(df)):\n",
    "            if pd.isna(df.atc_code[i]):\n",
    "                df.atc_code[i] = df.atc_code[i-1]\n",
    "                df.atc_name[i] = df.atc_name[i-1]\n",
    "        \n",
    "        return df\n",
    "\n",
    "# List of root ATC codes to scrape\n",
    "atc_root_codes = ['A', 'B', 'C', 'D', 'G', 'H', 'J', 'L', 'M', 'N', 'P', 'R', 'S', 'V']\n",
    "\n",
    "# Process each ATC root code individually\n",
    "for atc_root in atc_root_codes:\n",
    "    print(f'Processing ATC root: {atc_root}')\n",
    "    cache_or_generate_data(f'who_atc_{atc_root}', scrape_atc_data, str(atc_root))\n",
    "\n",
    "# Read the files produced by scrape_atc_data()\n",
    "combined_atc_data = pd.concat([load_cached_data(f'who_atc_{atc_root}') for atc_root in atc_root_codes if load_cached_data(f'who_atc_{atc_root}') is not None], ignore_index=True)\n",
    "\n",
    "# Write them to a CSV file. Generate file name from current date in year-month-day format.\n",
    "output_file_name = os.path.join(output_directory, f'WHO ATC-DDD {datetime.now().strftime(\"%Y-%m-%d\")}.csv')\n",
    "print(f'Writing results to {output_file_name}.')\n",
    "if os.path.exists(output_file_name):\n",
    "    print('Warning: file already exists. Will be overwritten.')\n",
    "combined_atc_data.to_csv(output_file_name, index=False)\n",
    "\n",
    "# Finish execution\n",
    "print('Script execution completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
